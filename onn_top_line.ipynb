{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- encoding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import imp\n",
    "import sys\n",
    "\n",
    "import utils\n",
    "imp.reload(utils)\n",
    "from utils.utils import rescale, softmax\n",
    "\n",
    "import dataset\n",
    "imp.reload(dataset)\n",
    "from dataset import MNIST, Feature\n",
    "\n",
    "import model\n",
    "imp.reload(model)   # 不这样reload，调试的时候修改引用的py文件是没有作用的\n",
    "from model import Net_1, Net_2, Optim, AccFunc, Population, CrossEntropyLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_size = 100\n",
    "layer1_node = 20\n",
    "layer2_node = 50\n",
    "output_size = 2\n",
    "\n",
    "batch_size = 1000\n",
    "epoch = 1000\n",
    "\n",
    "individual_num = 20\n",
    "mu_p = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### data pre-processing ###############\n",
    "\n",
    "train_set = MNIST('mnist', 'train', (10, 10))\n",
    "test_set = MNIST('mnist', 't10k', (10, 10))\n",
    "\n",
    "'''10 class\n",
    "train_feature = Feature(train_set.data, kernel_size=(4,4), stride=(3,3))\n",
    "train_fv = train_feature._data['images'].reshape(-1, 100)         # 10 class, 100 input\n",
    "rescale(train_fv, 30, 250, False)\n",
    "train_label = train_feature._data['labels']\n",
    "input_train_data = train_feature.cut_into_batch(batch_size=1000, vector=train_fv, labels=train_label)\n",
    "\n",
    "test_feature = Feature(test_set.data, kernel_size=(4,4), stride=(3,3))\n",
    "test_fv = test_feature._data['images'].reshape(-1, 100)           # 10 class, 100 input\n",
    "rescale(test_fv, 30, 250, False)\n",
    "test_label = test_feature._data['labels']\n",
    "input_test_data = test_feature.cut_into_batch(batch_size=1000, vector=test_fv, labels=test_label)\n",
    "'''\n",
    "\n",
    "train_feature = Feature(train_set.data, kernel_size=(4,4), stride=(3,3))\n",
    "train_fv, train_label = train_feature.extract_num_class(0, 1)\n",
    "# print(train_fv.shape)\n",
    "# print(train_label.shape)\n",
    "train_fv = train_fv.reshape(-1, 100)\n",
    "rescale(train_fv, 30, 250, False)\n",
    "input_train_data = train_feature.cut_into_batch(batch_size=1000, vector=train_fv, labels=train_label, num_class=output_size, one_hot=True)\n",
    "\n",
    "test_feature = Feature(test_set.data, kernel_size=(4,4), stride=(3,3))\n",
    "test_fv, test_label = test_feature.extract_num_class(0, 1)\n",
    "test_fv = test_fv.reshape(-1, 100)\n",
    "rescale(test_fv, 30, 250, False)\n",
    "input_test_data = test_feature.cut_into_batch(batch_size=1000, vector=test_fv, labels=test_label, num_class=output_size, one_hot=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  60 114  10\n    0   0   0   0   0   2  76 238 210 112   0   0   0   0   0  70 230 142\n   93 176   6   0   0   0  32 187  62   0   0 207  40   0   0   0 136  94\n    0   0   9 198  27   0   0   0 149  43   0  28 142  72   0   0   0   0\n  150 174 158 177  61   0   0   0   0   0  35 135 101  11   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0  21  32\n    0   0   0   0   0   0   0   3 176 134   0   0   0   0   0   0   0  93\n  197  14   0   0   0   0   0   0  17 215  65   0   0   0   0   0   0   4\n  172 141   1   0   0   0   0   0   0  72 226  31   0   0   0   0   0   0\n    4 176 143   0   0   0   0   0   0   0   6 184  58   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  90  56   0   0\n    0   0   0   0   0   1 156 115   0   0   0   0   0   0   0   1 153 166\n    4   0   0   0   0   0   0   0  91 211   8   0   0   0   0   0   0   0\n   91 216   9   0   0   0   0   0   0   0  66 250  52   0   0   0   0   0\n    0   0   6 229 114   0   0   0   0   0   0   0   0  97  69   0   0   0\n    0   0   0   0   0   0   0   0   0   0]]\n[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   9  74   0\n    0   0   0   0   0   0   0  63  83   0   0   0   0   0   0   0   0 142\n   22   0   0   0   0   0   0   0   7 167   6   0   0   0   0   0   0   0\n   64 118   0   0   0   0   0   0   0   0 143  49   0   0   0   0   0   0\n    0   8 196  12   0   0   0   0   0   0   0  10  86   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  43 106   2   0\n    0   0   0   0   0  35 193 223  58   1   0   0   0   0   6 175 248 174\n  211  52   0   0   0   0  48 237  82   1  94 202   6   0   0   0  81 144\n    0   2  98 223  17   0   0   0 113 150  41 115 241 107   0   0   0   0\n   60 228 243 244 154  13   0   0   0   0   2  51 126  66   2   0   0   0\n    0   0   0   0   0   0   0   0   0   0]\n [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   5  29   0\n    0   0   0   0   0   0   0  86 170   1   0   0   0   0   0   0   0 189\n   99   0   0   0   0   0   0   0  22 233  19   0   0   0   0   0   0   0\n   95 168   1   0   0   0   0   0   0   2 173  84   0   0   0   0   0   0\n    0  10 210  10   0   0   0   0   0   0   0   9 153   0   0   0   0   0\n    0   0   0   0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "input_test_data[0][1].shape\n",
    "print(train_fv[:3])\n",
    "print(test_fv[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw(title):\n",
    "\n",
    "    threshold = net.get_parameters()\n",
    "    layer1_th = threshold[0]\n",
    "    layer2_th = threshold[1]\n",
    "\n",
    "    # print(layer1_th.shape)\n",
    "    # print(layer2_th.shape)\n",
    "\n",
    "    all_th = np.hstack((layer1_th.flatten(), layer2_th.flatten())).tolist()\n",
    "    plt.hist(all_th)\n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### model define ###############\n",
    "\n",
    "net = Net_1(input_size, layer1_node, output_size)\n",
    "# net = Net_2(input_size, layer1_node, layer2_node, output_size)\n",
    "\n",
    "individuals = Population(individual_num, input_size, layer1_node, output_size)\n",
    "\n",
    "acc_func = AccFunc()\n",
    "criterion = CrossEntropyLoss()\n",
    "optimizer = Optim(net)"
   ]
  },
  {
   "source": [
    "############### train ###############\n",
    "\n",
    "max_acc = 0.\n",
    "for _ in range(epoch):\n",
    "    print(\"Epoch: %d\" % _)\n",
    "    for i, (images, labels) in enumerate(input_train_data):\n",
    "        individual_loss = []\n",
    "        acc_collecter = []\n",
    "        for param in individuals.get_all_params():\n",
    "            # change different individual\n",
    "            net.set_parameters(param)\n",
    "            \n",
    "            outputs = net(images)\n",
    "            outputs = softmax(outputs)\n",
    "            # print(outputs[:3])\n",
    "            # print()\n",
    "            # print(outputs.shape)    # shape=(batch_size, 1)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            # print(loss)\n",
    "            individual_loss.append(loss)   # label should one-hot\n",
    "            \n",
    "            acc = acc_func(outputs, labels)\n",
    "            acc_collecter.append(acc)\n",
    "\n",
    "        ############ Top 5 acc and record #############\n",
    "        acc_collecter = np.array(acc_collecter)\n",
    "        acc_sorted = abs(np.sort(-acc_collecter))\n",
    "        print(acc_sorted[:5])\n",
    "        if acc_sorted[0] > max_acc:\n",
    "            individuals.record_best(np.argsort(-acc_collecter)[0])\n",
    "            max_acc = acc_sorted[0]\n",
    "\n",
    "        ############ GA #############\n",
    "        individuals.select(individual_loss) # selection good individuals\n",
    "        # get next generation\n",
    "        popu_params = individuals.get_all_params()\n",
    "        new_popu_params = []\n",
    "        for i in range(0, len(popu_params), 2):\n",
    "            child_1, child_2 = individuals.crossover(popu_params[i], popu_params[i+1])\n",
    "            new_popu_params.append(child_1)\n",
    "            new_popu_params.append(child_2)\n",
    "        \n",
    "        # mutation\n",
    "        popu_p = np.random.rand(individual_num)\n",
    "        for i in range(individual_num):\n",
    "            if popu_p[i] > mu_p:\n",
    "                # print('Epoch: %d, individual: %d' % (_, i))\n",
    "                individuals.mutation(new_popu_params[i])\n",
    "\n",
    "        # update population\n",
    "        individuals.update_popu(new_popu_params)\n",
    "\n",
    "############ Save best parameters #############\n",
    "individuals.save_best(max_acc, 'log')"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 71,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch: 0\n",
      "[0.624 0.596 0.59  0.587 0.574]\n",
      "[0.679 0.657 0.643 0.575 0.572]\n",
      "[0.641 0.638 0.597 0.576 0.573]\n",
      "[0.58  0.573 0.567 0.559 0.546]\n",
      "[0.587 0.584 0.565 0.558 0.548]\n",
      "[0.629 0.622 0.616 0.579 0.546]\n",
      "[0.59  0.59  0.588 0.568 0.562]\n",
      "[0.612 0.572 0.566 0.561 0.55 ]\n",
      "[0.602 0.587 0.571 0.567 0.562]\n",
      "[0.636 0.614 0.573 0.561 0.556]\n",
      "[0.668 0.603 0.579 0.569 0.562]\n",
      "[0.633 0.61  0.571 0.562 0.545]\n",
      "[0.63609023 0.62105263 0.60902256 0.60451128 0.5443609 ]\n",
      "Epoch: 1\n",
      "[0.632 0.608 0.606 0.596 0.593]\n",
      "[0.631 0.588 0.581 0.575 0.556]\n",
      "[0.667 0.653 0.621 0.588 0.554]\n",
      "[0.612 0.6   0.592 0.579 0.566]\n",
      "[0.607 0.579 0.563 0.556 0.555]\n",
      "[0.616 0.603 0.603 0.565 0.555]\n",
      "[0.603 0.592 0.586 0.582 0.577]\n",
      "[0.639 0.578 0.559 0.552 0.545]\n",
      "[0.598 0.598 0.572 0.566 0.565]\n",
      "[0.634 0.601 0.574 0.565 0.549]\n",
      "[0.608 0.548 0.536 0.533 0.525]\n",
      "[0.658 0.608 0.586 0.573 0.555]\n",
      "[0.66466165 0.65864662 0.64360902 0.58195489 0.57894737]\n",
      "save completed\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}