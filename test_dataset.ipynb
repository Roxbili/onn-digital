{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3c5be2b06ed6dbdf00174833742462bf4dfeb93002accc42ac9edec65c60a8dd"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- encoding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import imp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from dataset import MNIST, Feature\n",
    "# import model\n",
    "# imp.reload(model)   # 不这样reload，调试的时候修改引用的py文件是没有作用的\n",
    "# from model import Net, LossFunc, Optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "input_size = 100\n",
    "hidden_size = 20\n",
    "hidden2_size = 20\n",
    "num_classes = 10\n",
    "num_epochs = 100\n",
    "batch_size = 1000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, hidden2_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.fc2 = nn.Linear(hidden_size, hidden2_size)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden2_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        # out = self.fc2(out)\n",
    "        # out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, hidden2_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### data pre-processing ###############\n",
    "\n",
    "train_set = MNIST('mnist', 'train', (10, 10))\n",
    "test_set = MNIST('mnist', 't10k', (10, 10))\n",
    "\n",
    "train_feature = Feature(train_set.data, kernel_size=(4,4), stride=(3,3))\n",
    "# train_v, _ = train_feature.calc_feature_vector()\n",
    "# train_compress_v = train_feature.compress()\n",
    "# train_fv, train_label = train_feature.encoding(train_v, is_round=True)\n",
    "# train_feature.hist(save_path='log/train_hist.png', data=train_fv)\n",
    "# input_train_data = train_feature.cut_into_batch(batch_size=1000, vector=train_fv, labels=train_label)\n",
    "train_fv = train_feature._data['images'].reshape(-1, 100)\n",
    "train_label = train_feature._data['labels']\n",
    "\n",
    "test_feature = Feature(test_set.data, kernel_size=(4,4), stride=(3,3))\n",
    "# test_v, _ = test_feature.calc_feature_vector()\n",
    "# test_compress_v = test_feature.compress()\n",
    "# test_fv, test_label = test_feature.encoding(test_v, is_round=True)\n",
    "# test_feature.hist(save_path='log/test_hist.png', data=test_fv)\n",
    "# input_test_data = test_feature.cut_into_batch(batch_size=1000, vector=test_fv, labels=test_label)\n",
    "test_fv = test_feature._data['images'].reshape(-1, 100)\n",
    "test_label = test_feature._data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## 创建pytoch使用的数据集 ##############\n",
    "\n",
    "class MyDataset(Dataset): #创建自己的类：MyDataset,这个类是继承的torch.utils.data.Dataset\n",
    "    def __init__(self, input, labels): #初始化一些需要传入的参数\n",
    "        self.input = input\n",
    "        self.labels = labels\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        return self.input[index], self.labels[index]\n",
    " \n",
    "    def __len__(self): #这个函数也必须要写，它返回的是数据集的长度，也就是多少张图片，要和loader的长度作区分\n",
    "        return self.labels.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset(train_fv, train_label)\n",
    "test_data = MyDataset(test_fv, test_label)\n",
    "\n",
    "#然后就是调用DataLoader和刚刚创建的数据集，来创建dataloader，这里提一句，loader的长度是有多少个batch，所以和batch_size有关\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.long().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.long().to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}