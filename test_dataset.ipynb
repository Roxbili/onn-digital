{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('pytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3c5be2b06ed6dbdf00174833742462bf4dfeb93002accc42ac9edec65c60a8dd"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- encoding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import imp\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from dataset import MNIST, Feature\n",
    "# import model\n",
    "# imp.reload(model)   # 不这样reload，调试的时候修改引用的py文件是没有作用的\n",
    "# from model import Net, LossFunc, Optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "input_size = 9\n",
    "hidden_size = 5\n",
    "hidden2_size = 5\n",
    "num_classes = 4\n",
    "num_epochs = 200\n",
    "batch_size = 1000\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fully connected neural network with one hidden layer\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, hidden2_size, num_classes):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        # self.fc2 = nn.Linear(hidden_size, hidden2_size)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden2_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        # out = self.fc2(out)\n",
    "        # out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNet(input_size, hidden_size, hidden2_size, num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "############### data pre-processing ###############\n",
    "\n",
    "train_set = MNIST('mnist', 'train', (10, 10))\n",
    "test_set = MNIST('mnist', 't10k', (10, 10))\n",
    "\n",
    "train_feature = Feature(train_set.data, kernel_size=(4,4), stride=(3,3))\n",
    "train_v, train_label = train_feature.calc_feature_vector()\n",
    "# train_fv, train_label = train_feature.encoding(train_v, is_round=True)\n",
    "# train_feature.hist(save_path='log/train_hist.png', data=train_fv)\n",
    "# input_train_data = train_feature.cut_into_batch(batch_size=1000, vector=train_fv, labels=train_label)\n",
    "train_fv, train_label = train_feature.extract_num_class([0, 1, 4, 7], train_v, train_label)\n",
    "train_fv = train_fv.reshape(-1, 9)\n",
    "# train_fv = train_feature._data['images'].reshape(-1, 100)\n",
    "# train_label = train_feature._data['labels']\n",
    "\n",
    "test_feature = Feature(test_set.data, kernel_size=(4,4), stride=(3,3))\n",
    "test_v, test_label = test_feature.calc_feature_vector()\n",
    "# test_fv, test_label = test_feature.encoding(test_v, is_round=True)\n",
    "# test_feature.hist(save_path='log/test_hist.png', data=test_fv)\n",
    "# input_test_data = test_feature.cut_into_batch(batch_size=1000, vector=test_fv, labels=test_label)\n",
    "test_fv, test_label = test_feature.extract_num_class([0, 1, 4, 7], test_v, test_label)\n",
    "test_fv = test_fv.reshape(-1, 9)\n",
    "# test_fv = test_feature._data['images'].reshape(-1, 100)\n",
    "# test_label = test_feature._data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## 创建pytoch使用的数据集 ##############\n",
    "\n",
    "class MyDataset(Dataset): #创建自己的类：MyDataset,这个类是继承的torch.utils.data.Dataset\n",
    "    def __init__(self, input, labels): #初始化一些需要传入的参数\n",
    "        self.input = input\n",
    "        self.labels = labels\n",
    " \n",
    "    def __getitem__(self, index):\n",
    "        return self.input[index], self.labels[index]\n",
    " \n",
    "    def __len__(self): #这个函数也必须要写，它返回的是数据集的长度，也就是多少张图片，要和loader的长度作区分\n",
    "        return self.labels.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MyDataset(train_fv, train_label)\n",
    "test_data = MyDataset(test_fv, test_label)\n",
    "\n",
    "#然后就是调用DataLoader和刚刚创建的数据集，来创建dataloader，这里提一句，loader的长度是有多少个batch，所以和batch_size有关\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "42.4731,  92.3064],\n",
      "        ...,\n",
      "        [291.8690, 168.3273,   0.0000, 437.3553,   0.0000],\n",
      "        [166.1488, 189.1833,  43.5988, 321.5572,  96.5184],\n",
      "        [223.3181, 208.2751,  45.5937, 374.0827,   0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[208.9508,   3.6280, 264.7027, 178.7487, 140.0381],\n",
      "        [294.0529, 192.8374,  42.5436, 437.2769,   0.0000],\n",
      "        [285.2379, 202.7866, 257.6888, 410.6824, 214.2082],\n",
      "        ...,\n",
      "        [447.5952, 514.2845, 450.5875, 801.5633,   9.4979],\n",
      "        [186.0947, 149.3399, 325.3672, 268.5378,  38.8889],\n",
      "        [314.5437, 236.3480, 183.8544, 493.2284,  92.2422]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[173.2906, 113.4692, 261.4665, 239.2951,  89.9574],\n",
      "        [145.9613,  97.5169, 209.2927, 202.2225, 102.1147],\n",
      "        [201.4291, 102.0430, 146.8692, 272.5321, 157.9138],\n",
      "        ...,\n",
      "        [251.4661, 216.9987,  77.8828, 373.5628,   0.0000],\n",
      "        [433.8625, 151.9418, 116.4420, 524.2882, 281.6606],\n",
      "        [155.3663, 413.1805, 114.8552, 462.6748,   0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[312.9189, 192.5967, 119.8120, 457.9780, 196.5672],\n",
      "        [393.7367, 341.6711,  63.5862, 670.4402,   0.0000],\n",
      "        [250.9061, 163.0548, 191.7247, 358.4048, 227.5603],\n",
      "        ...,\n",
      "        [266.8525, 366.6056, 104.4257, 532.7350,   0.0000],\n",
      "        [170.6111, 138.7942, 247.7661, 253.2981, 128.3561],\n",
      "        [160.4333,  36.1015, 242.5659, 162.5546,  88.2412]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[274.6311, 238.3794, 243.0484, 442.8745,  32.8105],\n",
      "        [188.6337, 149.2092, 305.8369, 265.6027,  74.6733],\n",
      "        [208.3747,  72.7926, 107.0137, 252.0626, 126.7211],\n",
      "        ...,\n",
      "        [195.7474, 114.3275, 290.1929, 257.0320, 116.0103],\n",
      "        [165.6688,  80.7677, 261.6263, 198.3305,  21.3545],\n",
      "        [339.2290, 303.2716, 318.3225, 555.9068,   0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[167.3331, 148.0698, 271.6133, 252.0543,  76.0562],\n",
      "        [ 78.7700, 164.6699,   0.0000, 228.3747,   0.0000],\n",
      "        [156.0850,  40.0237,  54.3394, 180.2864, 185.6509],\n",
      "        ...,\n",
      "        [305.8188, 383.7557, 278.9514, 606.2635,  48.6302],\n",
      "        [ 18.3544,   0.0000,  14.2080,   1.7771,  41.6151],\n",
      "        [ 60.9797,   7.3189, 112.0481,  55.3770,  72.5822]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[128.2925, 121.7188,  51.4109, 228.8550,  57.4386],\n",
      "        [481.4360, 251.8824, 405.3301, 624.7118, 122.2208],\n",
      "        [109.1216, 257.5438,  19.2900, 292.3531,   0.0000],\n",
      "        ...,\n",
      "        [122.2162, 336.6078,  66.2643, 396.8123,  78.5696],\n",
      "        [245.9416,  96.3478, 135.6553, 306.6797, 184.8711],\n",
      "        [410.1109, 616.1851, 465.3780, 860.0390,  35.5313]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[398.1669, 598.0292, 479.3578, 810.3795,   0.0000],\n",
      "        [244.1374, 432.4840, 121.5432, 582.2302,   0.0000],\n",
      "        [319.0708, 266.0778, 215.4059, 509.8646,   0.0000],\n",
      "        ...,\n",
      "        [279.3820, 440.3380, 207.0676, 644.5460,  69.4938],\n",
      "        [115.2581, 136.5800, 227.5593, 193.6712,  10.0044],\n",
      "        [379.4073, 355.0662, 100.1684, 677.9277,  80.6936]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[383.1130, 206.6795, 173.1652, 513.9504, 312.0133],\n",
      "        [320.0132, 692.8485, 385.0286, 827.6902,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        ...,\n",
      "        [317.6653, 379.8134, 288.7703, 594.7188,   0.0000],\n",
      "        [306.9550, 513.5575, 318.2426, 671.9286,   0.0000],\n",
      "        [ 94.1490, 133.0028,  64.8991, 204.3915,   0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[327.2434, 519.6572, 368.0388, 694.8315,   0.0000],\n",
      "        [198.9926, 197.8421,  70.6712, 335.3077,   0.0000],\n",
      "        [  3.5050,   0.0000,   0.0000,   0.0000,  83.4918],\n",
      "        ...,\n",
      "        [276.6225, 300.6530, 239.5567, 495.0717,   0.0000],\n",
      "        [412.0638, 647.8062, 420.3251, 885.3045,   0.0000],\n",
      "        [303.7467, 198.4895, 148.8664, 449.4008, 236.1656]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[400.7817, 208.1072, 173.6473, 523.9720, 382.2181],\n",
      "        [126.4284, 477.0831, 182.5056, 480.9592,   0.0000],\n",
      "        [156.5983, 234.8677, 163.6764, 297.7237,   0.0000],\n",
      "        ...,\n",
      "        [229.8315, 265.3984,   0.0000, 445.9822,  85.2580],\n",
      "        [212.1941, 267.2723,   0.0000, 434.1081, 112.6488],\n",
      "        [200.7697, 237.6502,  55.8597, 400.5621,   0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[  0.0000, 230.0735,   0.0000, 176.0066,   0.0000],\n",
      "        [417.3148, 279.0031, 152.9091, 612.8312, 114.8848],\n",
      "        [158.0172, 145.9558, 123.6224, 271.1746, 198.0074],\n",
      "        ...,\n",
      "        [167.5249,   6.4294, 210.4973, 145.9020,  84.7733],\n",
      "        [213.1368, 269.7245,  66.4874, 431.1676,   0.0000],\n",
      "        [106.4034, 165.9838,   0.0000, 203.4769,   0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[ 170.9849,  198.7835,    0.0000,  348.3640,    0.0000],\n",
      "        [ 599.7049,  640.4357,  648.4116, 1023.8702,  141.6167],\n",
      "        [   0.0000,   13.0080,    0.0000,    0.0000,    0.0000],\n",
      "        ...,\n",
      "        [ 334.8813,  518.7828,  398.4981,  700.7206,    0.0000],\n",
      "        [ 245.9390,  188.1897,  350.5185,  357.2357,  157.5751],\n",
      "        [ 239.2583,  236.2334,  365.7395,  373.9228,   74.0779]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Epoch [141/200], Step [10/25], Loss: 0.3164\n",
      "tensor([[355.4270, 259.7814, 166.2043, 544.1864,   0.0000],\n",
      "        [150.9058,  61.8694, 238.1505, 175.2843,  48.0921],\n",
      "        [331.3562, 272.9729, 209.0701, 534.7221,   0.0000],\n",
      "        ...,\n",
      "        [441.0626, 289.9482, 262.4333, 613.7778, 388.3421],\n",
      "        [156.9698, 142.9790, 218.4407, 246.1585, 110.3142],\n",
      "        [360.6804, 145.8368, 162.0878, 434.8493, 327.6591]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[152.5672, 227.9312,   0.0000, 306.6647,   0.0000],\n",
      "        [403.0368, 152.1948, 300.0083, 474.0022, 378.5459],\n",
      "        [273.2958, 308.1539, 261.2662, 489.3893,  38.4377],\n",
      "        ...,\n",
      "        [118.4692,  16.4446, 162.6811, 113.5214,  58.5452],\n",
      "        [ 85.5725, 153.2443,   0.0000, 229.3613,   0.0000],\n",
      "        [210.7794, 308.4205,  35.3461, 454.8715,  12.9039]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[277.0251, 110.7458,  70.2632, 339.1992,  41.2430],\n",
      "        [196.5161, 155.7737,  14.9306, 281.8772,   0.0000],\n",
      "        [338.1715, 159.5305,   4.8091, 449.5893,  15.4382],\n",
      "        ...,\n",
      "        [195.0369, 285.6334,  71.0172, 388.0363,   0.0000],\n",
      "        [496.6342, 285.3769, 375.4369, 671.6486, 417.4280],\n",
      "        [320.1566, 200.7135, 455.3710, 427.9960, 151.1836]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[156.6583, 235.8119,   5.5014, 361.0190,  70.6153],\n",
      "        [325.9178, 179.5202, 113.3462, 453.5910,   0.0000],\n",
      "        [103.2984, 286.5363, 140.2818, 295.8845,   0.0000],\n",
      "        ...,\n",
      "        [220.2413, 234.3072, 384.5526, 350.4914,  57.2691],\n",
      "        [277.5399,  31.4171, 362.3517, 259.7780, 179.5539],\n",
      "        [481.8840, 503.1417, 262.5702, 833.7988,   0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[ 23.1194,   0.0000,   0.0000,   0.0000,  52.1521],\n",
      "        [257.0654,  80.4592, 238.8529, 299.4610, 228.9523],\n",
      "        [182.1304,  87.2339,  45.2770, 242.3055, 173.9786],\n",
      "        ...,\n",
      "        [185.7073,  39.7337, 268.6971, 189.0212,  99.2540],\n",
      "        [470.6726, 257.3475, 304.0681, 629.7081, 312.6155],\n",
      "        [158.3932,  35.8602, 220.1016, 161.1890,  52.1545]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[133.6720,  87.5953, 220.8492, 176.8982,  31.3108],\n",
      "        [254.1078, 226.6152, 375.4888, 381.2632, 139.4825],\n",
      "        [355.9769, 383.6704, 441.3862, 613.9537, 164.9662],\n",
      "        ...,\n",
      "        [275.9477, 158.8322, 105.3353, 375.0989, 318.2078],\n",
      "        [219.2868, 149.8557, 340.0989, 307.8764,  98.2331],\n",
      "        [ 45.8174,   0.0000,  26.2572,   0.7930, 102.0499]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[239.6066, 182.9725, 243.0305, 335.0634,   0.0000],\n",
      "        [ 95.9577,   0.0000,  90.1487,  41.3543, 167.8623],\n",
      "        [155.0641,  40.8304, 236.5527, 162.5589,  76.2531],\n",
      "        ...,\n",
      "        [221.6302, 121.0219,  95.2217, 305.5711, 209.5831],\n",
      "        [455.3336, 398.4674, 247.6726, 730.7752, 188.3616],\n",
      "        [ 95.4313, 148.2182,  35.5426, 216.6845,   0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[183.1064,  52.9139, 109.8485, 216.2614, 151.4697],\n",
      "        [174.0572,   4.9045, 236.4831, 150.4520, 125.7812],\n",
      "        [354.5023, 253.9394, 207.4968, 529.5110,   3.9475],\n",
      "        ...,\n",
      "        [445.5401, 299.6494, 140.2010, 640.4617, 232.4275],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,  15.1742],\n",
      "        [199.7761,  80.3887,  46.7440, 252.9825,   0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[184.9471, 270.3206,  75.1270, 370.3794,   0.0000],\n",
      "        [265.9122,  96.7274, 148.8103, 324.2611, 267.8954],\n",
      "        [281.6707, 273.6516,   0.0000, 498.1599,   0.0000],\n",
      "        ...,\n",
      "        [177.4090, 141.2150,   0.0000, 305.4597,  12.5706],\n",
      "        [101.7206,  20.2829, 141.5892, 102.1490,  39.3210],\n",
      "        [269.6403, 231.3321, 334.6847, 398.8842, 192.2446]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[419.4651, 402.7589, 333.4977, 702.4643,   0.0000],\n",
      "        [435.6013, 267.5140, 226.6685, 592.8768, 366.4769],\n",
      "        [ 95.5236, 123.1262,   0.0000, 163.3335,   0.0000],\n",
      "        ...,\n",
      "        [340.0120, 210.7174, 132.8314, 440.6296,   0.0000],\n",
      "        [295.9604, 179.5042, 176.9519, 423.4615, 208.2791],\n",
      "        [266.9197, 157.7870, 365.4276, 339.6281, 110.3912]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Epoch [141/200], Step [20/25], Loss: 0.2425\n",
      "tensor([[302.2396, 147.6290, 120.9095, 408.8138, 246.8160],\n",
      "        [170.3322, 137.5138,  57.9261, 284.6962,  53.2181],\n",
      "        [383.4294, 243.6874, 146.8764, 554.1276,   0.0000],\n",
      "        ...,\n",
      "        [162.4261, 128.0264,  10.2911, 228.2181,   0.0000],\n",
      "        [378.8390, 192.4424, 199.9427, 483.6796, 226.7646],\n",
      "        [378.9507, 173.7906, 232.1197, 467.2037, 320.9676]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[104.6696,  34.1017, 153.5972, 113.3153,  27.9691],\n",
      "        [264.1481, 406.5537, 305.6828, 543.5497,   0.0000],\n",
      "        [258.9744, 435.1252, 269.8784, 556.9496,   0.0000],\n",
      "        ...,\n",
      "        [142.9106, 163.4908,   0.0000, 295.7960,  58.5225],\n",
      "        [322.3386, 158.8858, 190.8519, 425.0429, 236.9480],\n",
      "        [220.9322, 192.4505,   0.0000, 381.6403,   8.7262]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[213.2589, 239.0850,  44.7041, 407.1144, 215.8023],\n",
      "        [151.7720,  75.3420,  14.5343, 155.2528,   0.0000],\n",
      "        [  0.0000, 152.0286, 127.7562, 102.9040,   0.0000],\n",
      "        ...,\n",
      "        [ 24.4197,   0.0000,  47.9465,  16.2008,  53.2966],\n",
      "        [350.6787, 211.3081, 105.4198, 505.6676, 245.5541],\n",
      "        [197.6281, 233.8366,   0.0000, 403.9700,   7.9586]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[292.8747, 203.4858,   0.0000, 452.3680,   0.0000],\n",
      "        [365.4622, 317.0887, 214.7708, 581.7708,  42.9739],\n",
      "        [ 65.3662, 138.9198,  23.7858, 181.9846,   0.0000],\n",
      "        ...,\n",
      "        [152.4481,  75.3348,  51.1776, 200.8243, 164.9432],\n",
      "        [302.6940, 398.0197, 158.1180, 619.6284,   0.0000],\n",
      "        [106.7916,   5.8952, 155.1597,  94.3787,  97.9000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[230.9398, 212.0236, 314.6411, 355.1282, 127.4439],\n",
      "        [195.1740,   0.0000, 241.4169, 163.1107, 141.4527],\n",
      "        [ 77.6260,   0.0000,  82.7838,  48.2605,  73.3444],\n",
      "        ...,\n",
      "        [195.8770,   0.0000, 245.6001, 164.7478, 143.0342],\n",
      "        [163.4405,  66.3344, 262.5030, 190.4575,  85.0812],\n",
      "        [224.5434, 168.7912,  20.5854, 363.6096, 127.0488]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[172.4184, 166.7086, 258.9793, 273.5073, 104.5961],\n",
      "        [155.5671,   0.0000, 189.3304, 129.8043,  90.9500],\n",
      "        [100.6106, 187.7418,   0.0000, 266.9402,   0.0000],\n",
      "        ...,\n",
      "        [287.7578, 163.4015,  23.1473, 425.1096,  85.6662],\n",
      "        [283.7451, 210.5189,  87.4859, 461.0800,  19.7910],\n",
      "        [297.9084, 283.6981, 161.4963, 513.7791, 101.5966]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[ 81.0427, 149.9809,  82.2746, 204.8337,  48.7472],\n",
      "        [272.4073, 262.9804, 197.1784, 442.8601,   0.0000],\n",
      "        [ 16.4016,  21.8790,   0.0000,   9.5445,   0.0000],\n",
      "        ...,\n",
      "        [232.4430, 148.7895,   0.0000, 354.0338,   0.0000],\n",
      "        [181.4001, 316.0634,  96.3085, 389.8227,   0.0000],\n",
      "        [243.5969, 165.7976,  89.1431, 364.0729,   0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[178.3172, 130.6178,  97.9696, 279.3747, 232.3340],\n",
      "        [167.0253, 354.1331,  73.5319, 431.7080,   0.0000],\n",
      "        [500.6003, 509.6680, 490.0460, 851.1045, 116.6855],\n",
      "        ...,\n",
      "        [237.8673,  58.3469,  51.2798, 273.0358, 180.8951],\n",
      "        [456.5272, 491.8804, 150.4613, 821.0383,   0.0000],\n",
      "        [ 20.1110, 125.5322,   0.0000, 153.7648,   0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[3.0679e+02, 1.6102e+02, 8.4349e+01, 4.2732e+02, 2.7803e+02],\n",
      "        [1.1000e+02, 6.1175e+01, 1.7962e+02, 1.3634e+02, 0.0000e+00],\n",
      "        [2.1680e+02, 2.1498e+02, 9.4533e+00, 3.9685e+02, 5.1444e+01],\n",
      "        ...,\n",
      "        [1.5348e+02, 1.7168e+02, 0.0000e+00, 3.1094e+02, 8.9186e+01],\n",
      "        [2.5689e+02, 1.6680e+02, 1.7920e+02, 3.6438e+02, 2.6440e+01],\n",
      "        [1.1181e+02, 7.1037e+01, 0.0000e+00, 1.8334e+02, 1.6638e-01]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[ 78.6785,  85.9828,   0.0000, 155.7280,   0.0000],\n",
      "        [271.4450, 226.0833, 100.8826, 436.8634,   0.0000],\n",
      "        [295.4665, 297.6758, 267.5226, 497.6140,  23.4214],\n",
      "        ...,\n",
      "        [279.7744, 343.2239,  85.3803, 530.4414,   5.1015],\n",
      "        [297.8198, 120.4016,  12.3018, 383.4760,   0.0000],\n",
      "        [ 55.4922,   0.0000,  90.0633,  41.0852,  80.6560]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[427.1407, 317.2851, 212.8477, 649.3842, 333.1302],\n",
      "        [267.4142, 412.6299, 142.7754, 583.8129,   0.0000],\n",
      "        [364.9166, 478.9742, 185.0524, 706.2463,   0.0000],\n",
      "        ...,\n",
      "        [ 89.3042,  42.6117, 144.7009, 108.4970,  19.8080],\n",
      "        [402.3840, 242.1014, 164.4317, 573.6619, 315.2376],\n",
      "        [168.6319, 239.2330,   0.0000, 374.8073,  33.3069]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[ 99.7589,  88.4852,  77.6045, 173.5506,  60.3647],\n",
      "        [ 52.1147, 204.1850, 147.2885, 182.6059,   0.0000],\n",
      "        [122.8797, 102.7858,   0.0000, 208.5687,   0.0000],\n",
      "        ...,\n",
      "        [361.7514, 311.9920, 134.1988, 547.8149,   0.0000],\n",
      "        [203.8454, 101.9496, 293.5176, 257.6305, 108.2291],\n",
      "        [235.3513, 219.2175, 142.6453, 395.3518, 137.9646]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[378.0806, 536.1469, 300.0267, 771.3139,   0.0000],\n",
      "        [216.8656, 201.6767, 120.3630, 372.6573,   0.0000],\n",
      "        [106.2365, 131.3299,   0.0000, 209.7795,   0.0000],\n",
      "        ...,\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,  39.2524],\n",
      "        [188.0971,  76.0502,   1.4935, 233.0820, 146.8437],\n",
      "        [204.4975, 130.4297,  64.5301, 303.2418, 167.9688]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[224.7149, 424.5426, 145.3253, 535.4454,   0.0000],\n",
      "        [196.3662, 114.8673, 288.3911, 257.5269, 116.3534],\n",
      "        [336.1826, 229.0419,  23.3371, 502.5675, 169.4188],\n",
      "        ...,\n",
      "        [315.0979, 186.6772,   0.0000, 446.7028,   0.0000],\n",
      "        [182.7484, 389.5105,  26.0960, 467.4223,   0.0000],\n",
      "        [167.5632, 240.3092, 164.8879, 340.9272,   0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[197.1355,   0.0000, 264.5144, 163.3264, 163.5539],\n",
      "        [263.6445, 199.3055, 179.8931, 406.3282,  25.1490],\n",
      "        [437.5732, 369.3352, 163.6446, 714.0390,   0.0000],\n",
      "        ...,\n",
      "        [218.5567, 319.8731,  70.2093, 484.9680, 146.0874],\n",
      "        [ 97.3127,  38.0897, 176.9833, 109.9477,  48.4077],\n",
      "        [155.6862, 220.0731, 281.2919, 293.2607,  30.2286]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Epoch [142/200], Step [10/25], Loss: 0.2294\n",
      "tensor([[274.8198, 213.0219, 395.9645, 392.2027, 152.5289],\n",
      "        [104.6016, 286.0906, 140.1615, 295.9177,   0.0000],\n",
      "        [212.9509,  43.5805, 301.4746, 217.2587, 160.5850],\n",
      "        ...,\n",
      "        [301.9556, 399.7964, 167.3397, 631.6758,   0.0000],\n",
      "        [151.7395, 185.5799,  39.9244, 302.8556,   8.8649],\n",
      "        [327.7719, 193.4613, 230.0770, 437.8026, 222.5289]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[164.7676,  42.8622,  27.7101, 132.2666,  12.3373],\n",
      "        [303.5892, 319.1039, 150.0547, 565.8923, 165.5449],\n",
      "        [179.5012,  89.0888,   0.0000, 244.0743,  24.3720],\n",
      "        ...,\n",
      "        [112.9240, 189.2159,  49.2637, 271.5907, 117.1150],\n",
      "        [121.8301,   0.0000, 155.1519, 102.2587,  78.3092],\n",
      "        [175.2680,  41.4675, 251.6897, 181.2512,  88.6012]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[253.3217, 349.5414, 134.2743, 531.1473, 156.4763],\n",
      "        [ 15.0685, 211.1201,   0.0000, 166.8696,   0.0000],\n",
      "        [329.3811, 381.8455, 168.1343, 601.0217,   0.0000],\n",
      "        ...,\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000,   3.0207],\n",
      "        [380.5796, 455.4485, 130.3590, 697.3403,   0.0000],\n",
      "        [124.8196,   0.0000,  67.4611,  69.7401,  92.2515]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[149.3902,  39.6511,  18.5516, 175.0948, 149.6490],\n",
      "        [140.0969, 129.7281,   0.0000, 258.6251,   0.0000],\n",
      "        [131.9420, 271.6888,  18.1308, 362.9893,   0.0000],\n",
      "        ...,\n",
      "        [522.4625, 678.5820, 475.5735, 997.4259,  12.8515],\n",
      "        [136.3061,  34.2707,   9.8698, 156.1741,  95.4570],\n",
      "        [ 90.4246, 119.3987,  30.5748, 193.2825,  45.4548]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[141.9176, 104.4516, 137.2426, 221.9395,  78.6268],\n",
      "        [372.6504, 167.6480, 202.0012, 466.2599, 338.2142],\n",
      "        [356.6433, 274.8910, 115.2118, 569.3237, 193.0712],\n",
      "        ...,\n",
      "        [337.1116, 302.2103, 110.4252, 564.7314,  40.2956],\n",
      "        [430.4196, 301.6703, 207.7825, 633.4174, 307.5345],\n",
      "        [491.3262, 654.1717, 228.8509, 974.9437,   0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[244.4869, 192.7228,   7.1626, 405.2963,  34.1730],\n",
      "        [251.1597, 281.6183, 123.7302, 470.3224,  38.2693],\n",
      "        [239.5056, 321.7242,  56.6481, 497.8005,   0.0000],\n",
      "        ...,\n",
      "        [278.3421, 202.5412,  20.9802, 429.6388,   0.0000],\n",
      "        [136.1283, 223.1773,   0.0000, 315.4507,   0.0000],\n",
      "        [282.4767, 155.7552,  27.2217, 405.7520, 222.5414]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[164.0538, 105.2093, 252.6937, 225.6499,  54.0171],\n",
      "        [181.3703, 246.8770, 287.9604, 338.3281,  85.9355],\n",
      "        [375.9528, 311.8254, 184.5773, 586.6893, 235.5133],\n",
      "        ...,\n",
      "        [147.5311,  77.6458, 249.2875, 186.2672,  62.2619],\n",
      "        [  0.0000,   0.0000,   0.0000,   0.0000, 117.5521],\n",
      "        [331.7075, 459.5611, 220.0493, 658.5537,   0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[442.0874, 498.1805, 506.2215, 778.6391,  57.1416],\n",
      "        [300.9421, 320.2357, 345.1712, 529.7578,   0.0000],\n",
      "        [247.6913, 305.4314,  76.2149, 448.2588,   0.0000],\n",
      "        ...,\n",
      "        [358.2170, 287.8947, 185.4582, 548.8148, 281.6660],\n",
      "        [241.6131, 357.4680, 170.8116, 505.7538,   0.0000],\n",
      "        [347.1682, 313.2758, 137.0820, 597.1219, 239.7869]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[210.1025, 236.6073, 205.9691, 376.8796,  71.0596],\n",
      "        [ 87.4793, 170.3435,   4.8800, 201.7273,   0.0000],\n",
      "        [240.8930, 225.2989, 204.4710, 393.7583, 206.3903],\n",
      "        ...,\n",
      "        [162.4123,  23.5394, 222.4620, 155.5034,  79.6405],\n",
      "        [116.2501, 160.7558, 216.3714, 215.0496,  27.2202],\n",
      "        [321.8908, 185.1901,  26.3748, 464.5561, 235.3824]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "tensor([[223.7618, 276.1572, 285.8787, 368.8722,   0.0000],\n",
      "        [129.3115, 138.5741,   4.0555, 241.7045, 145.9658],\n",
      "        [265.9711, 122.9324, 119.6465, 342.2826, 238.4459],\n",
      "        ...,\n",
      "        [169.8467,  78.7090,  58.4170, 219.5669, 216.0672],\n",
      "        [118.2001,  67.7245, 206.6033, 152.0475,  42.9425],\n",
      "        [  0.0000,  46.5597,   0.0000,   0.4931,   0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n",
      "Epoch [142/200], Step [20/25], Loss: 0.2916\n",
      "tensor([[217.4684, 219.1845, 362.4207, 341.9173,  45.2558],\n",
      "        [257.5773, 136.2179,  76.4921, 351.2270, 175.6386],\n",
      "        [402.8278, 334.8710, 174.3951, 654.2825, 330.9598],\n",
      "        ...,\n",
      "        [270.2310, 413.3805, 346.1839, 551.6657,  28.7670],\n",
      "        [309.7920, 269.2510, 395.8484, 480.6462, 164.8717],\n",
      "        [187.7643,  42.3053, 153.1044, 197.3351, 138.9537]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-147d9f579833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# print(outputs.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# print(labels.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Backward and optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m-> 1048\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2690\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/pytorch/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1670\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1673\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.long().to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # print(outputs.shape)\n",
    "        # print(labels.shape)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.float().to(device)\n",
    "        labels = labels.long().to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the 10000 test images: {} %'.format(100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parm={}\n",
    "for name,parameters in model.named_parameters():\n",
    "    print(name,':',parameters.size())\n",
    "    parm[name]=parameters.detach().numpy()\n",
    "\n",
    "fc1_weight = parm['fc1.weight']\n",
    "fc1_bias = parm['fc1.bias']\n",
    "# print(fc1_weight)\n",
    "# print(fc1_bias)\n",
    "\n",
    "for i in range(fc1_bias.shape[0]):\n",
    "    bias = fc1_bias[i]\n",
    "    weight = fc1_weight[i]\n",
    "    print('Threshold...')\n",
    "    print(bias / weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}